\section{Limitations and Future Work}
Although the implementation of the bi-partite network random walker model in the context of Wikipedia provides novel insights on the quality of articles and expertise of editors, a number of points need further investigations. It remains unclear why the correlation of the method with the grand-truth for editors increases as a convex function of time (see Figure \ref{fig:rhotime} lower panel). We have proposed that it could be the result of the shape of the input matrix $\mathbf{M}$ : usually categories have factors more editors than articles and therefore more information accumulated over time is needed to adequately rank the editors. This hypothesis could be tested further by screening more Wikipedia categories according to a broader ratio of editors and articles. 

For the rank correlations of editors with the grand-truth $\alpha$ is systematically found equal to $0$ (see Figure \ref{fig:landscape} lower panel). This suggests that in some situations, even the zero\textsuperscript{th} order iteration is sufficient. While we have not performed this test, it would be interesting to evaluate of the bi-partite network walker method catches up with the grand-truth as a function of iterations.

According to the original philosophy of the reflection method \cite{hidalgo2007}, the capabilities of producing entities (e.g., countries) that constrain the production are not observable. We believe this is feasible in the case open collaboration. We can for instance identify what an editor do best to improve an article, among the five metrics (ratio of mark-up to readable text, number of headings, article length, citations per article length, and outgoing intrawiki links) we have used to assess the quality of an article.

We believe there are two further directions to improve our results. First we have followed the philosophy of the reflection method and we have used rankings by defaults. However, the bi-partite network random walker method provides absolute values, which might have a meaning in the context of open collaboration. Second, we took the very simplest information for the input matrix $\mathbf{M}$ (i.e. whether an editor has modified an article). We wonder how the performance of the method might change if richer information is incorporated in the matrix (e.g., number of edits). Would it improve performance or on the contrary reduce it ?


%We only look at the ranking not at the real quality/expertise values ? Can we learn more the real values about the gap between articles/editors ?

%{\bf [initially in discussion but we have not figure to support this point]} : When we use $\mathbf{M}$ %which is a binary version of $\mathbf{\hat{M}}$ we achieve better results. Knowing that editor touched an article, is more informative than knowing the edit count.  Wikipedia even acknowledges its {\it editcountitis, Compteurd√©dite} with the essay \cite{editcountitis}Wikipedia has never been explicitly gamified, but some editors are immediately attracted to tracking their performance. This leaves us with an overused, and perverse-incentive metric. In fact what we have shown is that edit truly is meaningless when it comes to predicting editor investment and quality. This is a departure from the economics domain, where the best fits for GDP are only in the positive / positive $\alpha$-$\beta$ quadrant.In the cases where maximizing $\alpha$, $\beta$ solution spaces are linear we get a kind of single variable characteristic of a category. 


