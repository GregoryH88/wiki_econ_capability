\section{Introduction}
From product reviews to online collaboration platforms, the Web is increasingly populated with knowledge contributed by the crowds. One positive aspect is the immediate sharing of information, which in turn helps others take more informed decisions about the quality of a product, the cleanliness of a restaurant, or whether it is worth buying a book. But the reliability of the contribution is limited by the level of expertise of the contributing editor. In many cases, repeated contributions of the same type by several individuals (e.g., reviews) average out idiosyncracies, assuming that individuals do not influence each others. 
Many crowdsourcing mechanisms have been designed to exploit the "wisdom of the crowds". \cite{https://en.wikipedia.org/wiki/The_Wisdom_of_Crowds}. The reliability and quality of a contribution is however more critical  when the same contribution is not necessarily repeated, or repeatable. Open source collaboration projects face such an issue - unlike averaging star-ratings - determining quality of Wikipedia articles, or opensource codebases is hard. Yet these open collaboration projects can achieve remarkable quality \cite{nature paper}. This is achieved through {\it peer-production} a labor organization, which mainly relies on two main ingredients : (i) task self-selection and (ii) peer-review : individuals choose the tasks they believe they are more qualified for, and they review the work of others also according to their skills \cite{benkler2002}. These principles have been initially used for open source software development \cite{raymond1999}, for which it is possible to ultimately test the quality of the work by executing the code. However, for natural language knowledge, like  Wikipedia, there is no "machine" to execute the code. Without the testability afforded to Raymond, the quality of the knowledge remains subjective, yet we retain similar inputs - a quantity of contributors working on a quantity of files.
Is there a way to assess relative quality of both articles and editors by only considering which editors have touched which articles on Wikipedia? This is the problem we are addressing in this paper.