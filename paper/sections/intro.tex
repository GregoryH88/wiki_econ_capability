\section{Introduction}
As knowledge is increasingly produced collectively on collaborative platforms such as the Wikipedia Encyclopedia, Github for open source software code, or Twitter for information filtering \cite{}, it becomes ever more important to understand how high quality content can be achieved from the ``wisdom of crowds". While  high quality can be achieved \cite{giles2005internet} and complex problems solved \cite{cooper2010}, the fine-grained mechanisms of performance remain largely unknown. It has been theorized that peer-production -- the labor organization underlying open collaboration -- emerges from the availability of cheap communication means like the Internet, and relies on two principal ingredients : (i) task self-selection and (ii) peer-review. In other words, individuals choose the tasks they believe they are more qualified for on the one hand, and on the other hand, they review the work of others also according to their skills \cite{benkler2002}. The rules of peer-production are very open and it is therefore very hard to understand precisely the motivations behind each individual contribution \cite{vonKrogh2012}. It is even harder to understand how idiosyncratic contributions 




From product reviews to online collaboration platforms, the Web is increasingly populated with knowledge contributed by the crowds. One positive aspect is the immediate sharing of information, which in turn helps others take more informed decisions about the quality of a product, the cleanliness of a restaurant, or whether it is worth buying a book. But the reliability of the contribution is limited by the level of expertise of the contributing editor. In many cases, repeated contributions of the same type by several individuals (e.g., reviews) average out idiosyncracies, assuming that individuals do not influence each others. 
Many crowdsourcing mechanisms have been designed to exploit the "wisdom of the crowds". \cite{https://en.wikipedia.org/wiki/The_Wisdom_of_Crowds}. The reliability and quality of a contribution is however more critical  when the same contribution is not necessarily repeated, or repeatable. Open source collaboration projects face such an issue - unlike averaging star-ratings - determining quality of Wikipedia articles, or opensource codebases is hard. Yet these open collaboration projects can achieve remarkable quality \cite{nature paper}. This is achieved through {\it peer-production} a labor organization, which mainly relies on two main ingredients : (i) task self-selection and (ii) peer-review : individuals choose the tasks they believe they are more qualified for, and they review the work of others also according to their skills \cite{benkler2002}. These principles have been initially used for open source software development \cite{raymond1999}, for which it is possible to ultimately test the quality of the work by executing the code. However, for natural language knowledge, like  Wikipedia, there is no "machine" to execute the code. Without the testability afforded to Raymond, the quality of the knowledge remains subjective, yet we retain similar inputs - a quantity of contributors working on a quantity of files.
Is there a way to assess relative quality of both articles and editors by only considering which editors have touched which articles on Wikipedia? This is the problem we are addressing in this paper.